{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb93888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "import torch.nn.init as init\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score\n",
    "from sklearn.metrics import f1_score,matthews_corrcoef\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "# from operator import itemgetter\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV,RepeatedStratifiedKFold,train_test_split,StratifiedKFold,KFold,cross_val_score,GroupKFold\n",
    "from sklearn.feature_selection import RFE,VarianceThreshold,RFECV,SelectFromModel\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.ensemble import BaggingClassifier,StackingClassifier,VotingClassifier,GradientBoostingClassifier,RandomForestClassifier,ExtraTreesClassifier,AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier,StackingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# from scipy.stats import pointbiserialr\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import make_scorer,matthews_corrcoef\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc1fecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>F1 (Toxic)</th>\n",
       "      <th>Precision (Toxic)</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>BA</th>\n",
       "      <th>AUC-ROC</th>\n",
       "      <th>AUC-PR</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model, Dataset, Preprocessing, F1 (Toxic), Precision (Toxic), Sensitivity, Specificity, BA, AUC-ROC, AUC-PR, MCC]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_index = pd.DataFrame(columns= [\"Model\",\"Dataset\",\"Preprocessing\",\"F1 (Toxic)\",\"Precision (Toxic)\",\"Sensitivity\",\n",
    "                                 \"Specificity\",\"BA\",\"AUC-ROC\",\"AUC-PR\",\"MCC\"])\n",
    "\n",
    "per_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7082a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(per_index,model_name,dataset,preprocessing,y_true,y_pred):\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    specificifty = tn/(tn+fp)\n",
    "    BA = (recall + specificifty)/2\n",
    "    x = (fp + tp)*(fp+tn)*(tp+fn)*(tn+fn)\n",
    "    mcc = (tp*tn - fp*fn) / pow(x,0.5)\n",
    "    f1 =  2 * (precision * recall) / (precision + recall)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    # 计算 AUC-PR\n",
    "    pr_auc = average_precision_score(y_true, y_pred)\n",
    "    data = {\n",
    "    \"Model\": model_name,\n",
    "    \"Dataset\": dataset,\n",
    "    \"Preprocessing\": preprocessing,\n",
    "    \"F1 (Toxic)\": f1,\n",
    "    \"Precision (Toxic)\": precision,\n",
    "    \"Sensitivity\": recall,\n",
    "    \"Specificity\": specificifty,\n",
    "    \"BA\": BA,\n",
    "    \"AUC-ROC\": roc_auc,\n",
    "    \"AUC-PR\": pr_auc,\n",
    "    \"MCC\": mcc\n",
    "    }\n",
    "    per_index = pd.concat([per_index, pd.DataFrame([data])], ignore_index=True)\n",
    "    return per_index\n",
    "def MCC_Loss(y_pred, y_true, class_weights):\n",
    "    tp = torch.sum((y_true * y_pred).float(), axis=0)\n",
    "    tn = torch.sum(((1 - y_true) * (1 - y_pred)).float(), axis=0)\n",
    "    fp = torch.sum(((1 - y_true) * y_pred).float(), axis=0)\n",
    "    fn = torch.sum((y_true * (1 - y_pred)).float(), axis=0)\n",
    "    x = (fp + tp) * (fp + tn) * (tp + fn) * (tn + fn) + 1e-7\n",
    "    mcc = (tp * tn - fp * fn) / torch.pow(x, 0.5)\n",
    "\n",
    "    mcc = torch.where(torch.isnan(mcc), torch.zeros_like(mcc), mcc)\n",
    "    print(class_weights)\n",
    "    weighted_mcc = mcc * class_weights  # 權重更新\n",
    "    return 1 - torch.mean(weighted_mcc)\n",
    "\n",
    "    \n",
    "def f1_loss(y_pred,y_true):\n",
    "    tp = torch.sum((y_true * y_pred).float(), axis=0)\n",
    "    tn = torch.sum(((1 - y_true) * (1 - y_pred)).float(), axis=0)\n",
    "    fp = torch.sum(((1 - y_true) * y_pred).float(), axis=0)\n",
    "    fn = torch.sum((y_true * (1 - y_pred)).float(), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + 1e-7)\n",
    "    r = tp / (tp + fn + 1e-7)\n",
    "\n",
    "    f1 = 2 * p * r / (p + r + 1e-7)\n",
    "    f1 = torch.where(torch.isnan(f1), torch.zeros_like(f1), f1)\n",
    "    return 1 - torch.mean(f1)\n",
    "def recall_loss(y_pred,y_true):\n",
    "    tp = torch.sum((y_true * y_pred).float(), axis=0)\n",
    "    tn = torch.sum(((1 - y_true) * (1 - y_pred)).float(), axis=0)\n",
    "    fp = torch.sum(((1 - y_true) * y_pred).float(), axis=0)\n",
    "    fn = torch.sum((y_true * (1 - y_pred)).float(), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + 1e-7)\n",
    "    r = tp / (tp + fn + 1e-7)\n",
    "\n",
    "    f1 = 2 * p * r / (p + r + 1e-7)\n",
    "    r = torch.where(torch.isnan(r), torch.zeros_like(r), r)\n",
    "    return 1 - torch.mean(r)\n",
    "def sp_loss(y_pred,y_true):\n",
    "    tp = torch.sum((y_true * y_pred).float(), axis=0)\n",
    "    tn = torch.sum(((1 - y_true) * (1 - y_pred)).float(), axis=0)\n",
    "    fp = torch.sum(((1 - y_true) * y_pred).float(), axis=0)\n",
    "    fn = torch.sum((y_true * (1 - y_pred)).float(), axis=0)\n",
    "\n",
    "    sp = tn / (tn + fp + 1e-7)\n",
    "\n",
    "\n",
    "    sp = torch.where(torch.isnan(sp), torch.zeros_like(sp), sp)\n",
    "    return 1 - torch.mean(sp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ecee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(y_train):\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    total_samples = len(y_train)\n",
    "    class_weights = total_samples / counts\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "class RNNModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, output_size, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "#         self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(hidden_size, 64)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.4)\n",
    "        self.fc2 = torch.nn.Linear(64, 8)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(8)\n",
    "        self.fc3 = torch.nn.Linear(8,1)\n",
    "\n",
    "     \n",
    "   \n",
    "\n",
    "\n",
    "        # 使用 He 初始化\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                init.kaiming_normal_(param, mode='fan_in', nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        init.kaiming_normal_(self.fc3.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "#         out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "#         out = self.dropout2(out)\n",
    "        out = self.relu(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b87c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = MCC_Loss(outputs, labels,calculate_class_weights(y_train))\n",
    "        l1_lambda = 0.000045\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss = loss + l1_lambda * l1_norm\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        predicted = torch.round(outputs).squeeze().cpu().detach().numpy()\n",
    "        predicted = predicted.flatten()\n",
    "        total += labels.size(0)\n",
    "        labels = labels.cpu().numpy()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        y_true.extend(labels)\n",
    "        y_pred.extend(predicted.tolist())\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    t_f1 = f1_score(y_true, y_pred)\n",
    "    t_mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    tcombined_score = (t_f1 + t_mcc) / 2\n",
    "    mtr = metrics.confusion_matrix(y_true, y_pred)\n",
    "    return train_loss,t_f1,t_mcc,tcombined_score,mtr\n",
    "\n",
    "def validate(model, val_loader, l2_lambda):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = MCC_Loss(outputs, labels,calculate_class_weights(y_train))\n",
    "            l1_lambda = 0.000045\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predicted = torch.round(outputs).squeeze().cpu().detach().numpy()\n",
    "            predicted = predicted.flatten()\n",
    "            total += labels.size(0)\n",
    "            labels = labels.cpu().numpy()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            y_true.extend(labels)\n",
    "            y_pred.extend(predicted.tolist())\n",
    "        val_loss = running_loss / len(val_loader)\n",
    "        val_f1 = f1_score(y_true, y_pred)\n",
    "        val_mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        mtr = metrics.confusion_matrix(y_true, y_pred)\n",
    "        combined_score = (val_f1 + val_mcc) / 2\n",
    "    return val_loss, val_f1, val_mcc, combined_score,mtr\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss =  MCC_Loss(outputs, labels,calculate_class_weights(y_test)) \n",
    "            l1_lambda = 0.000045\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm\n",
    "            running_loss += loss.item()\n",
    "            predicted = torch.round(outputs).squeeze().cpu().detach().numpy()\n",
    "            predicted = predicted.flatten()\n",
    "            total += labels.size(0)\n",
    "            labels = labels.cpu().numpy()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            y_true.extend(labels)\n",
    "            y_pred.extend(predicted.tolist())\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    mtr = metrics.confusion_matrix(y_true, y_pred)\n",
    "    return test_loss, f1,mcc,mtr,y_true,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44191626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_func(fingerprint1,fingerprint2):\n",
    "\n",
    "    print(np.array(fingerprint1).shape)\n",
    "    print(np.array(fingerprint2).shape)\n",
    "    # 將PCA結果轉換為DataFrame\n",
    "    train_pca_df = pd.DataFrame(data=fingerprint1, columns=['PC1', 'PC2'])\n",
    "    test_pca_df = pd.DataFrame(data=fingerprint2, columns=['PC1', 'PC2'])\n",
    "\n",
    "    # 繪製化學空間分佈圖\n",
    "    plt.scatter(train_pca_df['PC1'], train_pca_df['PC2'], c='blue', label='Non-Toxicity')\n",
    "    plt.scatter(test_pca_df['PC1'], test_pca_df['PC2'], c='red', label='Toxicity')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title('Chemical Space Distribution (Unbalanced)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b679d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "morgan_data = pd.read_csv(\"data/morgan_data.csv\",index_col=0)\n",
    "morgan_test = pd.read_csv(\"data/morgan_test.csv\",index_col=0)\n",
    "mordred_data= pd.read_csv(\"data/mordred_data.csv\",index_col=0)\n",
    "mordred_test= pd.read_csv(\"data/mordred_test.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0977682d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8815497e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25cfd7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if it is a value\n",
    "for col in mordred_data.columns:\n",
    "    if not pd.api.types.is_numeric_dtype(mordred_data[col]):\n",
    "        mordred_data[col] = pd.to_numeric(mordred_data[col], errors='coerce')\n",
    "        mordred_test[col] = pd.to_numeric(mordred_test[col], errors='coerce')\n",
    "# for col in mordred_test.columns:\n",
    "#     if not pd.api.types.is_numeric_dtype(mordred_test[col]):\n",
    "#         mordred_test[col] = pd.to_numeric(mordred_test[col], errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b31bfd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the nan value\n",
    "\n",
    "# mordred_data = mordred_data.dropna(axis=1, how='all')\n",
    "# mordred_test = mordred_test.dropna(axis=1, how='all')\n",
    "\n",
    "for col in mordred_data.columns:\n",
    "    if mordred_data[col].isna().sum() >= 1:\n",
    "        mordred_data[col].fillna(mordred_data[col].mean(), inplace=True)\n",
    "        mordred_test[col].fillna(mordred_test[col].mean(), inplace=True)\n",
    "# for col in mordred_test.columns:\n",
    "#     if mordred_test[col].isna().sum() >= 1:\n",
    "#         mordred_test[col].fillna(mordred_test[col].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d474b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the nan value\n",
    "\n",
    "mordred_data = mordred_data.dropna(axis=1, how='all')\n",
    "mordred_test = mordred_test.dropna(axis=1, how='all')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b66119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出mordred_data和mordred_test之间差异的列\n",
    "different_columns = set(mordred_data.columns) - set(mordred_test.columns)\n",
    "\n",
    "# 删除mordred_data中的差异列\n",
    "mordred_data = mordred_data.drop(columns=different_columns)\n",
    "\n",
    "# 找出mordred_test和mordred_data之间差异的列（因为在上一步中mordred_data的列已经改变了）\n",
    "different_columns = set(mordred_test.columns) - set(mordred_data.columns)\n",
    "\n",
    "# 删除mordred_test中的差异列\n",
    "mordred_test = mordred_test.drop(columns=different_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2df63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15c5fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "morgan_X_train,morgan_X_test,morgan_y_train,morgan_y_test = train_test_split(np.array(morgan_data.iloc[:,:-1]),np.array(morgan_data.iloc[:,-1]),stratify=np.array(morgan_data.iloc[:,-1]),test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bef5605d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4774\n",
       "1    1234\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(morgan_y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b339bf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to preserve 95.0% variance: 1106\n",
      "Average overlap ratio: 0.9406193123701279\n"
     ]
    }
   ],
   "source": [
    "pca = PCA()\n",
    "\n",
    "# PCA\n",
    "pca.fit(morgan_X_train)\n",
    "\n",
    "# 計算PCA各個主成份的方差解釋率\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# 累積方差解釋率\n",
    "cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# setting the threshold\n",
    "threshold = 0.95\n",
    "\n",
    "# 找到最小主成分数目\n",
    "n_components = np.where(cumulative_explained_variance_ratio >= threshold)[0][0] + 1\n",
    "\n",
    "print(f\"Number of components to preserve {threshold * 100}% variance: {n_components}\")\n",
    "\n",
    "pca_reduced = PCA(n_components=n_components)\n",
    "# 進行 PCA 轉換\n",
    "X_train = pca_reduced.fit_transform(morgan_X_train)\n",
    "# X_val1 = pca_reduced.transform(X_val)\n",
    "X_test = pca_reduced.transform(morgan_X_test)\n",
    "\n",
    "positive_indices = np.where(morgan_y_train == 1)[0]\n",
    "negative_indices = np.where(morgan_y_train == 0)[0]\n",
    "\n",
    "positive_samples = np.array(X_train)[positive_indices]\n",
    "negative_samples = np.array(X_train)[negative_indices]\n",
    "overlaps = []\n",
    "for i in range(positive_samples.shape[1]):  # for each feature\n",
    "    hist_pos, _ = np.histogram(positive_samples[:, i])\n",
    "    hist_neg, _ = np.histogram(negative_samples[:, i])\n",
    "    overlap = np.minimum(hist_pos, hist_neg).sum()  # calculate the overlap for this feature\n",
    "    overlaps.append(overlap)\n",
    "\n",
    "overlap_ratio = np.mean(overlaps) / positive_samples.shape[0]  # calculate the average overlap\n",
    "print(\"Average overlap ratio:\", overlap_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6441bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f105107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, n=1, Accuracy: 0.36 (+/- 0.02)\n",
      "k=2, n=2, Accuracy: 0.43 (+/- 0.02)\n",
      "k=3, n=3, Accuracy: 0.44 (+/- 0.02)\n",
      "k=4, n=4, Accuracy: 0.43 (+/- 0.02)\n",
      "k=5, n=5, Accuracy: 0.44 (+/- 0.02)\n",
      "k=6, n=6, Accuracy: 0.43 (+/- 0.02)\n",
      "k=7, n=7, Accuracy: 0.43 (+/- 0.02)\n",
      "k=8, n=8, Accuracy: 0.43 (+/- 0.02)\n",
      "k=9, n=9, Accuracy: 0.44 (+/- 0.02)\n",
      "k=10, n=10, Accuracy: 0.43 (+/- 0.02)\n",
      "k=11, n=11, Accuracy: 0.44 (+/- 0.01)\n",
      "k=12, n=12, Accuracy: 0.44 (+/- 0.01)\n",
      "k=13, n=13, Accuracy: 0.46 (+/- 0.02)\n",
      "k=14, n=14, Accuracy: 0.46 (+/- 0.02)\n",
      "k=15, n=15, Accuracy: 0.46 (+/- 0.02)\n",
      "k=16, n=16, Accuracy: 0.46 (+/- 0.01)\n",
      "k=17, n=17, Accuracy: 0.46 (+/- 0.01)\n",
      "k=18, n=18, Accuracy: 0.46 (+/- 0.02)\n",
      "k=19, n=19, Accuracy: 0.47 (+/- 0.02)\n",
      "k=20, n=20, Accuracy: 0.47 (+/- 0.01)\n",
      "k=21, n=21, Accuracy: 0.46 (+/- 0.01)\n",
      "k=22, n=22, Accuracy: 0.47 (+/- 0.01)\n",
      "k=23, n=23, Accuracy: 0.47 (+/- 0.01)\n",
      "k=24, n=24, Accuracy: 0.47 (+/- 0.02)\n",
      "k=25, n=25, Accuracy: 0.47 (+/- 0.01)\n",
      "k=26, n=26, Accuracy: 0.47 (+/- 0.01)\n",
      "k=27, n=27, Accuracy: 0.47 (+/- 0.02)\n",
      "k=28, n=28, Accuracy: 0.46 (+/- 0.02)\n",
      "k=29, n=29, Accuracy: 0.46 (+/- 0.02)\n",
      "k=30, n=30, Accuracy: 0.45 (+/- 0.02)\n",
      "k=31, n=31, Accuracy: 0.46 (+/- 0.02)\n",
      "k=32, n=32, Accuracy: 0.46 (+/- 0.02)\n",
      "k=33, n=33, Accuracy: 0.45 (+/- 0.02)\n",
      "k=34, n=34, Accuracy: 0.46 (+/- 0.03)\n",
      "k=35, n=35, Accuracy: 0.46 (+/- 0.02)\n",
      "k=36, n=36, Accuracy: 0.47 (+/- 0.03)\n",
      "k=37, n=37, Accuracy: 0.47 (+/- 0.03)\n",
      "k=38, n=38, Accuracy: 0.47 (+/- 0.02)\n",
      "k=39, n=39, Accuracy: 0.48 (+/- 0.03)\n",
      "k=40, n=40, Accuracy: 0.48 (+/- 0.03)\n",
      "k=41, n=41, Accuracy: 0.48 (+/- 0.03)\n",
      "k=42, n=42, Accuracy: 0.48 (+/- 0.03)\n",
      "k=43, n=43, Accuracy: 0.48 (+/- 0.03)\n",
      "k=44, n=44, Accuracy: 0.48 (+/- 0.02)\n",
      "k=45, n=45, Accuracy: 0.48 (+/- 0.02)\n",
      "k=46, n=46, Accuracy: 0.48 (+/- 0.03)\n",
      "k=47, n=47, Accuracy: 0.49 (+/- 0.04)\n",
      "k=48, n=48, Accuracy: 0.49 (+/- 0.03)\n",
      "k=49, n=49, Accuracy: 0.49 (+/- 0.03)\n",
      "k=50, n=50, Accuracy: 0.49 (+/- 0.03)\n",
      "k=51, n=51, Accuracy: 0.49 (+/- 0.03)\n",
      "k=52, n=52, Accuracy: 0.49 (+/- 0.04)\n",
      "k=53, n=53, Accuracy: 0.50 (+/- 0.04)\n",
      "k=54, n=54, Accuracy: 0.49 (+/- 0.03)\n",
      "k=55, n=55, Accuracy: 0.51 (+/- 0.03)\n",
      "k=56, n=56, Accuracy: 0.50 (+/- 0.04)\n",
      "k=57, n=57, Accuracy: 0.51 (+/- 0.03)\n",
      "k=58, n=58, Accuracy: 0.50 (+/- 0.03)\n",
      "k=59, n=59, Accuracy: 0.51 (+/- 0.02)\n",
      "k=60, n=60, Accuracy: 0.51 (+/- 0.02)\n",
      "k=61, n=61, Accuracy: 0.52 (+/- 0.02)\n",
      "k=62, n=62, Accuracy: 0.52 (+/- 0.03)\n",
      "k=63, n=63, Accuracy: 0.52 (+/- 0.03)\n",
      "k=64, n=64, Accuracy: 0.52 (+/- 0.03)\n",
      "k=65, n=65, Accuracy: 0.51 (+/- 0.03)\n",
      "k=66, n=66, Accuracy: 0.51 (+/- 0.03)\n",
      "k=67, n=67, Accuracy: 0.51 (+/- 0.03)\n",
      "k=68, n=68, Accuracy: 0.51 (+/- 0.04)\n",
      "k=69, n=69, Accuracy: 0.52 (+/- 0.03)\n",
      "k=70, n=70, Accuracy: 0.52 (+/- 0.03)\n",
      "k=71, n=71, Accuracy: 0.52 (+/- 0.03)\n",
      "k=72, n=72, Accuracy: 0.52 (+/- 0.03)\n",
      "k=73, n=73, Accuracy: 0.53 (+/- 0.03)\n",
      "k=74, n=74, Accuracy: 0.53 (+/- 0.03)\n",
      "k=75, n=75, Accuracy: 0.53 (+/- 0.02)\n",
      "k=76, n=76, Accuracy: 0.52 (+/- 0.03)\n",
      "k=77, n=77, Accuracy: 0.52 (+/- 0.03)\n",
      "k=78, n=78, Accuracy: 0.52 (+/- 0.03)\n",
      "k=79, n=79, Accuracy: 0.52 (+/- 0.04)\n",
      "k=80, n=80, Accuracy: 0.52 (+/- 0.03)\n",
      "k=81, n=81, Accuracy: 0.51 (+/- 0.03)\n",
      "k=82, n=82, Accuracy: 0.52 (+/- 0.02)\n",
      "k=83, n=83, Accuracy: 0.53 (+/- 0.03)\n",
      "k=84, n=84, Accuracy: 0.52 (+/- 0.03)\n",
      "k=85, n=85, Accuracy: 0.52 (+/- 0.03)\n",
      "k=86, n=86, Accuracy: 0.52 (+/- 0.03)\n",
      "k=87, n=87, Accuracy: 0.53 (+/- 0.03)\n",
      "k=88, n=88, Accuracy: 0.52 (+/- 0.03)\n",
      "k=89, n=89, Accuracy: 0.52 (+/- 0.03)\n",
      "k=90, n=90, Accuracy: 0.52 (+/- 0.03)\n",
      "k=91, n=91, Accuracy: 0.52 (+/- 0.03)\n",
      "k=92, n=92, Accuracy: 0.52 (+/- 0.03)\n",
      "k=93, n=93, Accuracy: 0.52 (+/- 0.02)\n",
      "k=94, n=94, Accuracy: 0.52 (+/- 0.02)\n",
      "k=95, n=95, Accuracy: 0.52 (+/- 0.03)\n",
      "k=96, n=96, Accuracy: 0.52 (+/- 0.03)\n",
      "k=97, n=97, Accuracy: 0.52 (+/- 0.03)\n",
      "k=98, n=98, Accuracy: 0.52 (+/- 0.03)\n",
      "k=99, n=99, Accuracy: 0.52 (+/- 0.02)\n",
      "k=100, n=100, Accuracy: 0.51 (+/- 0.03)\n",
      "k=101, n=101, Accuracy: 0.51 (+/- 0.03)\n",
      "k=102, n=102, Accuracy: 0.52 (+/- 0.02)\n",
      "k=103, n=103, Accuracy: 0.52 (+/- 0.03)\n",
      "k=104, n=104, Accuracy: 0.53 (+/- 0.02)\n",
      "k=105, n=105, Accuracy: 0.52 (+/- 0.02)\n",
      "k=106, n=106, Accuracy: 0.52 (+/- 0.02)\n",
      "k=107, n=107, Accuracy: 0.53 (+/- 0.02)\n",
      "k=108, n=108, Accuracy: 0.53 (+/- 0.03)\n",
      "k=109, n=109, Accuracy: 0.53 (+/- 0.03)\n",
      "k=110, n=110, Accuracy: 0.53 (+/- 0.03)\n",
      "k=111, n=111, Accuracy: 0.53 (+/- 0.03)\n",
      "k=112, n=112, Accuracy: 0.52 (+/- 0.03)\n",
      "k=113, n=113, Accuracy: 0.53 (+/- 0.03)\n",
      "k=114, n=114, Accuracy: 0.53 (+/- 0.03)\n",
      "k=115, n=115, Accuracy: 0.54 (+/- 0.03)\n",
      "k=116, n=116, Accuracy: 0.54 (+/- 0.04)\n",
      "k=117, n=117, Accuracy: 0.54 (+/- 0.03)\n",
      "k=118, n=118, Accuracy: 0.53 (+/- 0.03)\n",
      "k=119, n=119, Accuracy: 0.54 (+/- 0.03)\n",
      "k=120, n=120, Accuracy: 0.53 (+/- 0.02)\n",
      "k=121, n=121, Accuracy: 0.53 (+/- 0.02)\n",
      "k=122, n=122, Accuracy: 0.53 (+/- 0.02)\n",
      "k=123, n=123, Accuracy: 0.53 (+/- 0.03)\n",
      "k=124, n=124, Accuracy: 0.54 (+/- 0.03)\n",
      "k=125, n=125, Accuracy: 0.54 (+/- 0.03)\n",
      "k=126, n=126, Accuracy: 0.53 (+/- 0.03)\n",
      "k=127, n=127, Accuracy: 0.53 (+/- 0.03)\n",
      "k=128, n=128, Accuracy: 0.54 (+/- 0.03)\n",
      "k=129, n=129, Accuracy: 0.53 (+/- 0.03)\n",
      "k=130, n=130, Accuracy: 0.54 (+/- 0.03)\n",
      "k=131, n=131, Accuracy: 0.53 (+/- 0.04)\n",
      "k=132, n=132, Accuracy: 0.54 (+/- 0.03)\n",
      "k=133, n=133, Accuracy: 0.53 (+/- 0.04)\n",
      "k=134, n=134, Accuracy: 0.54 (+/- 0.04)\n",
      "k=135, n=135, Accuracy: 0.53 (+/- 0.04)\n",
      "k=136, n=136, Accuracy: 0.53 (+/- 0.04)\n",
      "k=137, n=137, Accuracy: 0.54 (+/- 0.03)\n",
      "k=138, n=138, Accuracy: 0.53 (+/- 0.04)\n",
      "k=139, n=139, Accuracy: 0.55 (+/- 0.02)\n",
      "k=140, n=140, Accuracy: 0.54 (+/- 0.03)\n",
      "k=141, n=141, Accuracy: 0.54 (+/- 0.04)\n",
      "k=142, n=142, Accuracy: 0.55 (+/- 0.03)\n",
      "k=143, n=143, Accuracy: 0.55 (+/- 0.03)\n",
      "k=144, n=144, Accuracy: 0.55 (+/- 0.03)\n",
      "k=145, n=145, Accuracy: 0.54 (+/- 0.03)\n",
      "k=146, n=146, Accuracy: 0.54 (+/- 0.04)\n",
      "k=147, n=147, Accuracy: 0.54 (+/- 0.03)\n",
      "k=148, n=148, Accuracy: 0.54 (+/- 0.03)\n",
      "k=149, n=149, Accuracy: 0.53 (+/- 0.03)\n",
      "k=150, n=150, Accuracy: 0.54 (+/- 0.03)\n",
      "k=151, n=151, Accuracy: 0.55 (+/- 0.03)\n",
      "k=152, n=152, Accuracy: 0.53 (+/- 0.02)\n",
      "k=153, n=153, Accuracy: 0.54 (+/- 0.03)\n",
      "k=154, n=154, Accuracy: 0.54 (+/- 0.03)\n",
      "k=155, n=155, Accuracy: 0.54 (+/- 0.03)\n",
      "k=156, n=156, Accuracy: 0.54 (+/- 0.03)\n",
      "k=157, n=157, Accuracy: 0.54 (+/- 0.03)\n",
      "k=158, n=158, Accuracy: 0.54 (+/- 0.03)\n",
      "k=159, n=159, Accuracy: 0.54 (+/- 0.04)\n",
      "k=160, n=160, Accuracy: 0.55 (+/- 0.03)\n",
      "k=161, n=161, Accuracy: 0.54 (+/- 0.03)\n",
      "k=162, n=162, Accuracy: 0.54 (+/- 0.03)\n",
      "k=163, n=163, Accuracy: 0.54 (+/- 0.03)\n",
      "k=164, n=164, Accuracy: 0.54 (+/- 0.03)\n",
      "k=165, n=165, Accuracy: 0.53 (+/- 0.03)\n",
      "k=166, n=166, Accuracy: 0.54 (+/- 0.03)\n",
      "k=167, n=167, Accuracy: 0.54 (+/- 0.03)\n",
      "k=168, n=168, Accuracy: 0.54 (+/- 0.03)\n",
      "k=169, n=169, Accuracy: 0.54 (+/- 0.03)\n",
      "k=170, n=170, Accuracy: 0.55 (+/- 0.03)\n",
      "k=171, n=171, Accuracy: 0.54 (+/- 0.03)\n",
      "k=172, n=172, Accuracy: 0.55 (+/- 0.03)\n",
      "k=173, n=173, Accuracy: 0.54 (+/- 0.03)\n",
      "k=174, n=174, Accuracy: 0.54 (+/- 0.02)\n",
      "k=175, n=175, Accuracy: 0.54 (+/- 0.04)\n",
      "k=176, n=176, Accuracy: 0.54 (+/- 0.03)\n",
      "k=177, n=177, Accuracy: 0.54 (+/- 0.03)\n",
      "k=178, n=178, Accuracy: 0.53 (+/- 0.03)\n",
      "k=179, n=179, Accuracy: 0.54 (+/- 0.04)\n",
      "k=180, n=180, Accuracy: 0.55 (+/- 0.03)\n",
      "k=181, n=181, Accuracy: 0.54 (+/- 0.03)\n",
      "k=182, n=182, Accuracy: 0.54 (+/- 0.03)\n",
      "k=183, n=183, Accuracy: 0.54 (+/- 0.03)\n",
      "k=184, n=184, Accuracy: 0.54 (+/- 0.03)\n",
      "k=185, n=185, Accuracy: 0.55 (+/- 0.03)\n",
      "k=186, n=186, Accuracy: 0.55 (+/- 0.03)\n",
      "k=187, n=187, Accuracy: 0.54 (+/- 0.03)\n",
      "k=188, n=188, Accuracy: 0.54 (+/- 0.03)\n",
      "k=189, n=189, Accuracy: 0.54 (+/- 0.02)\n",
      "k=190, n=190, Accuracy: 0.54 (+/- 0.03)\n",
      "k=191, n=191, Accuracy: 0.55 (+/- 0.02)\n",
      "k=192, n=192, Accuracy: 0.55 (+/- 0.03)\n",
      "k=193, n=193, Accuracy: 0.55 (+/- 0.03)\n",
      "k=194, n=194, Accuracy: 0.54 (+/- 0.03)\n",
      "k=195, n=195, Accuracy: 0.54 (+/- 0.03)\n",
      "k=196, n=196, Accuracy: 0.55 (+/- 0.02)\n",
      "k=197, n=197, Accuracy: 0.55 (+/- 0.03)\n",
      "k=198, n=198, Accuracy: 0.55 (+/- 0.03)\n",
      "k=199, n=199, Accuracy: 0.54 (+/- 0.03)\n",
      "k=200, n=200, Accuracy: 0.54 (+/- 0.03)\n",
      "k=201, n=201, Accuracy: 0.54 (+/- 0.03)\n",
      "k=202, n=202, Accuracy: 0.54 (+/- 0.03)\n",
      "k=203, n=203, Accuracy: 0.54 (+/- 0.02)\n",
      "k=204, n=204, Accuracy: 0.54 (+/- 0.02)\n",
      "k=205, n=205, Accuracy: 0.55 (+/- 0.02)\n",
      "k=206, n=206, Accuracy: 0.55 (+/- 0.02)\n",
      "k=207, n=207, Accuracy: 0.55 (+/- 0.02)\n",
      "k=208, n=208, Accuracy: 0.54 (+/- 0.03)\n",
      "k=209, n=209, Accuracy: 0.54 (+/- 0.03)\n",
      "k=210, n=210, Accuracy: 0.55 (+/- 0.02)\n",
      "k=211, n=211, Accuracy: 0.55 (+/- 0.03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=212, n=212, Accuracy: 0.55 (+/- 0.02)\n",
      "k=213, n=213, Accuracy: 0.54 (+/- 0.03)\n",
      "k=214, n=214, Accuracy: 0.54 (+/- 0.03)\n",
      "k=215, n=215, Accuracy: 0.54 (+/- 0.03)\n",
      "k=216, n=216, Accuracy: 0.54 (+/- 0.02)\n",
      "k=217, n=217, Accuracy: 0.54 (+/- 0.03)\n",
      "k=218, n=218, Accuracy: 0.54 (+/- 0.03)\n",
      "k=219, n=219, Accuracy: 0.54 (+/- 0.03)\n",
      "k=220, n=220, Accuracy: 0.55 (+/- 0.03)\n",
      "k=221, n=221, Accuracy: 0.55 (+/- 0.03)\n",
      "k=222, n=222, Accuracy: 0.54 (+/- 0.03)\n",
      "k=223, n=223, Accuracy: 0.54 (+/- 0.02)\n",
      "k=224, n=224, Accuracy: 0.55 (+/- 0.02)\n",
      "k=225, n=225, Accuracy: 0.54 (+/- 0.02)\n",
      "k=226, n=226, Accuracy: 0.55 (+/- 0.03)\n",
      "k=227, n=227, Accuracy: 0.54 (+/- 0.03)\n",
      "k=228, n=228, Accuracy: 0.55 (+/- 0.02)\n",
      "k=229, n=229, Accuracy: 0.54 (+/- 0.03)\n",
      "k=230, n=230, Accuracy: 0.54 (+/- 0.03)\n",
      "k=231, n=231, Accuracy: 0.54 (+/- 0.04)\n",
      "k=232, n=232, Accuracy: 0.55 (+/- 0.02)\n",
      "k=233, n=233, Accuracy: 0.53 (+/- 0.02)\n",
      "k=234, n=234, Accuracy: 0.55 (+/- 0.02)\n",
      "k=235, n=235, Accuracy: 0.54 (+/- 0.03)\n",
      "k=236, n=236, Accuracy: 0.54 (+/- 0.02)\n",
      "k=237, n=237, Accuracy: 0.54 (+/- 0.03)\n",
      "k=238, n=238, Accuracy: 0.54 (+/- 0.03)\n",
      "k=239, n=239, Accuracy: 0.54 (+/- 0.03)\n",
      "k=240, n=240, Accuracy: 0.54 (+/- 0.03)\n",
      "k=241, n=241, Accuracy: 0.54 (+/- 0.03)\n",
      "k=242, n=242, Accuracy: 0.53 (+/- 0.03)\n",
      "k=243, n=243, Accuracy: 0.53 (+/- 0.03)\n",
      "k=244, n=244, Accuracy: 0.54 (+/- 0.03)\n",
      "k=245, n=245, Accuracy: 0.53 (+/- 0.03)\n",
      "k=246, n=246, Accuracy: 0.54 (+/- 0.03)\n",
      "k=247, n=247, Accuracy: 0.53 (+/- 0.03)\n",
      "k=248, n=248, Accuracy: 0.54 (+/- 0.03)\n",
      "k=249, n=249, Accuracy: 0.54 (+/- 0.03)\n",
      "k=250, n=250, Accuracy: 0.53 (+/- 0.03)\n",
      "k=251, n=251, Accuracy: 0.54 (+/- 0.03)\n",
      "k=252, n=252, Accuracy: 0.53 (+/- 0.03)\n",
      "k=253, n=253, Accuracy: 0.54 (+/- 0.03)\n",
      "k=254, n=254, Accuracy: 0.54 (+/- 0.03)\n",
      "k=255, n=255, Accuracy: 0.53 (+/- 0.03)\n",
      "k=256, n=256, Accuracy: 0.54 (+/- 0.03)\n",
      "k=257, n=257, Accuracy: 0.53 (+/- 0.03)\n",
      "k=258, n=258, Accuracy: 0.54 (+/- 0.03)\n",
      "k=259, n=259, Accuracy: 0.54 (+/- 0.03)\n",
      "k=260, n=260, Accuracy: 0.54 (+/- 0.03)\n",
      "k=261, n=261, Accuracy: 0.54 (+/- 0.03)\n",
      "k=262, n=262, Accuracy: 0.54 (+/- 0.03)\n",
      "k=263, n=263, Accuracy: 0.54 (+/- 0.03)\n",
      "k=264, n=264, Accuracy: 0.54 (+/- 0.03)\n",
      "k=265, n=265, Accuracy: 0.54 (+/- 0.02)\n",
      "k=266, n=266, Accuracy: 0.54 (+/- 0.03)\n",
      "k=267, n=267, Accuracy: 0.53 (+/- 0.02)\n",
      "k=268, n=268, Accuracy: 0.53 (+/- 0.03)\n",
      "k=269, n=269, Accuracy: 0.53 (+/- 0.03)\n",
      "k=270, n=270, Accuracy: 0.54 (+/- 0.02)\n",
      "k=271, n=271, Accuracy: 0.54 (+/- 0.03)\n",
      "k=272, n=272, Accuracy: 0.54 (+/- 0.03)\n",
      "k=273, n=273, Accuracy: 0.54 (+/- 0.02)\n",
      "k=274, n=274, Accuracy: 0.54 (+/- 0.03)\n",
      "k=275, n=275, Accuracy: 0.54 (+/- 0.03)\n",
      "k=276, n=276, Accuracy: 0.53 (+/- 0.03)\n",
      "k=277, n=277, Accuracy: 0.53 (+/- 0.03)\n",
      "k=278, n=278, Accuracy: 0.53 (+/- 0.02)\n",
      "k=279, n=279, Accuracy: 0.53 (+/- 0.03)\n",
      "k=280, n=280, Accuracy: 0.53 (+/- 0.02)\n",
      "k=281, n=281, Accuracy: 0.53 (+/- 0.02)\n",
      "k=282, n=282, Accuracy: 0.54 (+/- 0.02)\n",
      "k=283, n=283, Accuracy: 0.53 (+/- 0.02)\n",
      "k=284, n=284, Accuracy: 0.53 (+/- 0.02)\n",
      "k=285, n=285, Accuracy: 0.53 (+/- 0.02)\n",
      "k=286, n=286, Accuracy: 0.54 (+/- 0.03)\n",
      "k=287, n=287, Accuracy: 0.53 (+/- 0.03)\n",
      "k=288, n=288, Accuracy: 0.54 (+/- 0.03)\n",
      "k=289, n=289, Accuracy: 0.54 (+/- 0.03)\n",
      "k=290, n=290, Accuracy: 0.53 (+/- 0.02)\n",
      "k=291, n=291, Accuracy: 0.53 (+/- 0.02)\n",
      "k=292, n=292, Accuracy: 0.53 (+/- 0.02)\n",
      "k=293, n=293, Accuracy: 0.54 (+/- 0.03)\n",
      "k=294, n=294, Accuracy: 0.53 (+/- 0.03)\n",
      "k=295, n=295, Accuracy: 0.53 (+/- 0.02)\n",
      "k=296, n=296, Accuracy: 0.53 (+/- 0.02)\n",
      "k=297, n=297, Accuracy: 0.54 (+/- 0.02)\n",
      "k=298, n=298, Accuracy: 0.53 (+/- 0.02)\n",
      "k=299, n=299, Accuracy: 0.53 (+/- 0.02)\n",
      "k=300, n=300, Accuracy: 0.53 (+/- 0.02)\n",
      "k=301, n=301, Accuracy: 0.54 (+/- 0.03)\n",
      "k=302, n=302, Accuracy: 0.54 (+/- 0.02)\n",
      "k=303, n=303, Accuracy: 0.54 (+/- 0.03)\n",
      "k=304, n=304, Accuracy: 0.54 (+/- 0.03)\n",
      "k=305, n=305, Accuracy: 0.53 (+/- 0.02)\n",
      "k=306, n=306, Accuracy: 0.53 (+/- 0.03)\n",
      "k=307, n=307, Accuracy: 0.53 (+/- 0.03)\n",
      "k=308, n=308, Accuracy: 0.54 (+/- 0.03)\n",
      "k=309, n=309, Accuracy: 0.53 (+/- 0.02)\n",
      "k=310, n=310, Accuracy: 0.54 (+/- 0.02)\n",
      "k=311, n=311, Accuracy: 0.54 (+/- 0.03)\n",
      "k=312, n=312, Accuracy: 0.54 (+/- 0.03)\n",
      "k=313, n=313, Accuracy: 0.53 (+/- 0.03)\n",
      "k=314, n=314, Accuracy: 0.54 (+/- 0.03)\n",
      "k=315, n=315, Accuracy: 0.53 (+/- 0.03)\n",
      "k=316, n=316, Accuracy: 0.53 (+/- 0.03)\n",
      "k=317, n=317, Accuracy: 0.53 (+/- 0.02)\n",
      "k=318, n=318, Accuracy: 0.54 (+/- 0.02)\n",
      "k=319, n=319, Accuracy: 0.52 (+/- 0.02)\n",
      "k=320, n=320, Accuracy: 0.53 (+/- 0.03)\n",
      "k=321, n=321, Accuracy: 0.53 (+/- 0.03)\n",
      "k=322, n=322, Accuracy: 0.54 (+/- 0.03)\n",
      "k=323, n=323, Accuracy: 0.54 (+/- 0.03)\n",
      "k=324, n=324, Accuracy: 0.54 (+/- 0.02)\n",
      "k=325, n=325, Accuracy: 0.53 (+/- 0.02)\n",
      "k=326, n=326, Accuracy: 0.54 (+/- 0.03)\n",
      "k=327, n=327, Accuracy: 0.54 (+/- 0.02)\n",
      "k=328, n=328, Accuracy: 0.54 (+/- 0.02)\n",
      "k=329, n=329, Accuracy: 0.54 (+/- 0.02)\n",
      "k=330, n=330, Accuracy: 0.53 (+/- 0.03)\n",
      "k=331, n=331, Accuracy: 0.53 (+/- 0.02)\n",
      "k=332, n=332, Accuracy: 0.54 (+/- 0.03)\n",
      "k=333, n=333, Accuracy: 0.53 (+/- 0.02)\n",
      "k=334, n=334, Accuracy: 0.54 (+/- 0.03)\n",
      "k=335, n=335, Accuracy: 0.53 (+/- 0.03)\n",
      "k=336, n=336, Accuracy: 0.53 (+/- 0.03)\n",
      "k=337, n=337, Accuracy: 0.53 (+/- 0.02)\n",
      "k=338, n=338, Accuracy: 0.54 (+/- 0.02)\n",
      "k=339, n=339, Accuracy: 0.54 (+/- 0.02)\n",
      "k=340, n=340, Accuracy: 0.54 (+/- 0.02)\n",
      "k=341, n=341, Accuracy: 0.53 (+/- 0.03)\n",
      "k=342, n=342, Accuracy: 0.54 (+/- 0.03)\n",
      "k=343, n=343, Accuracy: 0.54 (+/- 0.03)\n",
      "k=344, n=344, Accuracy: 0.53 (+/- 0.02)\n",
      "k=345, n=345, Accuracy: 0.53 (+/- 0.03)\n",
      "k=346, n=346, Accuracy: 0.54 (+/- 0.02)\n",
      "k=347, n=347, Accuracy: 0.53 (+/- 0.02)\n",
      "k=348, n=348, Accuracy: 0.53 (+/- 0.03)\n",
      "k=349, n=349, Accuracy: 0.54 (+/- 0.03)\n",
      "k=350, n=350, Accuracy: 0.53 (+/- 0.03)\n",
      "k=351, n=351, Accuracy: 0.54 (+/- 0.02)\n",
      "k=352, n=352, Accuracy: 0.53 (+/- 0.01)\n",
      "k=353, n=353, Accuracy: 0.53 (+/- 0.02)\n",
      "k=354, n=354, Accuracy: 0.53 (+/- 0.02)\n",
      "k=355, n=355, Accuracy: 0.53 (+/- 0.02)\n",
      "k=356, n=356, Accuracy: 0.54 (+/- 0.03)\n",
      "k=357, n=357, Accuracy: 0.53 (+/- 0.03)\n",
      "k=358, n=358, Accuracy: 0.54 (+/- 0.03)\n",
      "k=359, n=359, Accuracy: 0.54 (+/- 0.03)\n",
      "k=360, n=360, Accuracy: 0.53 (+/- 0.02)\n",
      "k=361, n=361, Accuracy: 0.54 (+/- 0.03)\n",
      "k=362, n=362, Accuracy: 0.55 (+/- 0.03)\n",
      "k=363, n=363, Accuracy: 0.54 (+/- 0.02)\n",
      "k=364, n=364, Accuracy: 0.54 (+/- 0.02)\n",
      "k=365, n=365, Accuracy: 0.53 (+/- 0.03)\n",
      "k=366, n=366, Accuracy: 0.54 (+/- 0.03)\n",
      "k=367, n=367, Accuracy: 0.54 (+/- 0.03)\n",
      "k=368, n=368, Accuracy: 0.54 (+/- 0.03)\n",
      "k=369, n=369, Accuracy: 0.54 (+/- 0.03)\n",
      "k=370, n=370, Accuracy: 0.53 (+/- 0.03)\n",
      "k=371, n=371, Accuracy: 0.54 (+/- 0.03)\n",
      "k=372, n=372, Accuracy: 0.54 (+/- 0.02)\n",
      "k=373, n=373, Accuracy: 0.54 (+/- 0.03)\n",
      "k=374, n=374, Accuracy: 0.54 (+/- 0.02)\n",
      "k=375, n=375, Accuracy: 0.53 (+/- 0.03)\n",
      "k=376, n=376, Accuracy: 0.54 (+/- 0.02)\n",
      "k=377, n=377, Accuracy: 0.54 (+/- 0.03)\n",
      "k=378, n=378, Accuracy: 0.54 (+/- 0.02)\n",
      "k=379, n=379, Accuracy: 0.54 (+/- 0.02)\n",
      "k=380, n=380, Accuracy: 0.53 (+/- 0.03)\n",
      "k=381, n=381, Accuracy: 0.54 (+/- 0.02)\n",
      "k=382, n=382, Accuracy: 0.54 (+/- 0.03)\n",
      "k=383, n=383, Accuracy: 0.54 (+/- 0.02)\n",
      "k=384, n=384, Accuracy: 0.54 (+/- 0.02)\n",
      "k=385, n=385, Accuracy: 0.54 (+/- 0.02)\n",
      "k=386, n=386, Accuracy: 0.54 (+/- 0.03)\n",
      "k=387, n=387, Accuracy: 0.54 (+/- 0.03)\n",
      "k=388, n=388, Accuracy: 0.54 (+/- 0.02)\n",
      "k=389, n=389, Accuracy: 0.53 (+/- 0.03)\n",
      "k=390, n=390, Accuracy: 0.55 (+/- 0.02)\n",
      "k=391, n=391, Accuracy: 0.55 (+/- 0.02)\n",
      "k=392, n=392, Accuracy: 0.54 (+/- 0.02)\n",
      "k=393, n=393, Accuracy: 0.54 (+/- 0.02)\n",
      "k=394, n=394, Accuracy: 0.54 (+/- 0.03)\n",
      "k=395, n=395, Accuracy: 0.54 (+/- 0.03)\n",
      "k=396, n=396, Accuracy: 0.54 (+/- 0.02)\n",
      "k=397, n=397, Accuracy: 0.54 (+/- 0.03)\n",
      "k=398, n=398, Accuracy: 0.54 (+/- 0.02)\n",
      "k=399, n=399, Accuracy: 0.54 (+/- 0.02)\n",
      "k=400, n=400, Accuracy: 0.55 (+/- 0.03)\n",
      "k=401, n=401, Accuracy: 0.54 (+/- 0.03)\n",
      "k=402, n=402, Accuracy: 0.54 (+/- 0.03)\n",
      "k=403, n=403, Accuracy: 0.54 (+/- 0.03)\n",
      "k=404, n=404, Accuracy: 0.54 (+/- 0.03)\n",
      "k=405, n=405, Accuracy: 0.54 (+/- 0.02)\n",
      "k=406, n=406, Accuracy: 0.54 (+/- 0.02)\n",
      "k=407, n=407, Accuracy: 0.54 (+/- 0.02)\n",
      "k=408, n=408, Accuracy: 0.54 (+/- 0.02)\n",
      "k=409, n=409, Accuracy: 0.54 (+/- 0.03)\n",
      "k=410, n=410, Accuracy: 0.54 (+/- 0.03)\n",
      "k=411, n=411, Accuracy: 0.54 (+/- 0.03)\n",
      "k=412, n=412, Accuracy: 0.55 (+/- 0.02)\n",
      "k=413, n=413, Accuracy: 0.55 (+/- 0.02)\n",
      "k=414, n=414, Accuracy: 0.54 (+/- 0.02)\n",
      "k=415, n=415, Accuracy: 0.54 (+/- 0.02)\n",
      "k=416, n=416, Accuracy: 0.54 (+/- 0.03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=417, n=417, Accuracy: 0.55 (+/- 0.03)\n",
      "k=418, n=418, Accuracy: 0.55 (+/- 0.03)\n",
      "k=419, n=419, Accuracy: 0.55 (+/- 0.03)\n",
      "k=420, n=420, Accuracy: 0.54 (+/- 0.01)\n",
      "k=421, n=421, Accuracy: 0.54 (+/- 0.03)\n",
      "k=422, n=422, Accuracy: 0.54 (+/- 0.03)\n",
      "k=423, n=423, Accuracy: 0.54 (+/- 0.03)\n",
      "k=424, n=424, Accuracy: 0.54 (+/- 0.03)\n",
      "k=425, n=425, Accuracy: 0.54 (+/- 0.02)\n",
      "k=426, n=426, Accuracy: 0.54 (+/- 0.03)\n",
      "k=427, n=427, Accuracy: 0.54 (+/- 0.02)\n",
      "k=428, n=428, Accuracy: 0.54 (+/- 0.02)\n",
      "k=429, n=429, Accuracy: 0.53 (+/- 0.03)\n",
      "k=430, n=430, Accuracy: 0.54 (+/- 0.02)\n",
      "k=431, n=431, Accuracy: 0.55 (+/- 0.02)\n",
      "k=432, n=432, Accuracy: 0.54 (+/- 0.03)\n",
      "k=433, n=433, Accuracy: 0.54 (+/- 0.02)\n",
      "k=434, n=434, Accuracy: 0.55 (+/- 0.03)\n",
      "k=435, n=435, Accuracy: 0.54 (+/- 0.03)\n",
      "k=436, n=436, Accuracy: 0.54 (+/- 0.02)\n",
      "k=437, n=437, Accuracy: 0.53 (+/- 0.04)\n",
      "k=438, n=438, Accuracy: 0.53 (+/- 0.03)\n",
      "k=439, n=439, Accuracy: 0.54 (+/- 0.03)\n",
      "k=440, n=440, Accuracy: 0.54 (+/- 0.03)\n",
      "k=441, n=441, Accuracy: 0.54 (+/- 0.03)\n",
      "k=442, n=442, Accuracy: 0.55 (+/- 0.03)\n",
      "k=443, n=443, Accuracy: 0.55 (+/- 0.03)\n",
      "k=444, n=444, Accuracy: 0.54 (+/- 0.03)\n",
      "k=445, n=445, Accuracy: 0.55 (+/- 0.02)\n",
      "k=446, n=446, Accuracy: 0.55 (+/- 0.03)\n",
      "k=447, n=447, Accuracy: 0.54 (+/- 0.02)\n",
      "k=448, n=448, Accuracy: 0.54 (+/- 0.03)\n",
      "k=449, n=449, Accuracy: 0.54 (+/- 0.03)\n",
      "k=450, n=450, Accuracy: 0.54 (+/- 0.02)\n",
      "k=451, n=451, Accuracy: 0.54 (+/- 0.03)\n",
      "k=452, n=452, Accuracy: 0.54 (+/- 0.03)\n",
      "k=453, n=453, Accuracy: 0.54 (+/- 0.03)\n",
      "k=454, n=454, Accuracy: 0.55 (+/- 0.03)\n",
      "k=455, n=455, Accuracy: 0.54 (+/- 0.03)\n",
      "k=456, n=456, Accuracy: 0.54 (+/- 0.03)\n",
      "k=457, n=457, Accuracy: 0.54 (+/- 0.02)\n",
      "k=458, n=458, Accuracy: 0.55 (+/- 0.03)\n",
      "k=459, n=459, Accuracy: 0.55 (+/- 0.03)\n",
      "k=460, n=460, Accuracy: 0.55 (+/- 0.03)\n",
      "k=461, n=461, Accuracy: 0.54 (+/- 0.02)\n",
      "k=462, n=462, Accuracy: 0.55 (+/- 0.03)\n",
      "k=463, n=463, Accuracy: 0.54 (+/- 0.03)\n",
      "k=464, n=464, Accuracy: 0.54 (+/- 0.03)\n",
      "k=465, n=465, Accuracy: 0.54 (+/- 0.02)\n",
      "k=466, n=466, Accuracy: 0.55 (+/- 0.03)\n",
      "k=467, n=467, Accuracy: 0.54 (+/- 0.03)\n",
      "k=468, n=468, Accuracy: 0.54 (+/- 0.03)\n",
      "k=469, n=469, Accuracy: 0.54 (+/- 0.03)\n",
      "k=470, n=470, Accuracy: 0.54 (+/- 0.02)\n",
      "k=471, n=471, Accuracy: 0.55 (+/- 0.03)\n",
      "k=472, n=472, Accuracy: 0.55 (+/- 0.02)\n",
      "k=473, n=473, Accuracy: 0.54 (+/- 0.02)\n",
      "k=474, n=474, Accuracy: 0.55 (+/- 0.03)\n",
      "k=475, n=475, Accuracy: 0.54 (+/- 0.03)\n",
      "k=476, n=476, Accuracy: 0.55 (+/- 0.03)\n",
      "k=477, n=477, Accuracy: 0.54 (+/- 0.03)\n",
      "k=478, n=478, Accuracy: 0.54 (+/- 0.03)\n",
      "k=479, n=479, Accuracy: 0.54 (+/- 0.03)\n",
      "k=480, n=480, Accuracy: 0.54 (+/- 0.02)\n",
      "k=481, n=481, Accuracy: 0.54 (+/- 0.03)\n",
      "k=482, n=482, Accuracy: 0.55 (+/- 0.03)\n",
      "k=483, n=483, Accuracy: 0.55 (+/- 0.03)\n",
      "k=484, n=484, Accuracy: 0.54 (+/- 0.04)\n",
      "k=485, n=485, Accuracy: 0.55 (+/- 0.03)\n",
      "k=486, n=486, Accuracy: 0.54 (+/- 0.03)\n",
      "k=487, n=487, Accuracy: 0.54 (+/- 0.03)\n",
      "k=488, n=488, Accuracy: 0.54 (+/- 0.02)\n",
      "k=489, n=489, Accuracy: 0.55 (+/- 0.03)\n",
      "k=490, n=490, Accuracy: 0.54 (+/- 0.03)\n",
      "k=491, n=491, Accuracy: 0.54 (+/- 0.02)\n",
      "k=492, n=492, Accuracy: 0.54 (+/- 0.02)\n",
      "k=493, n=493, Accuracy: 0.55 (+/- 0.03)\n",
      "k=494, n=494, Accuracy: 0.54 (+/- 0.03)\n",
      "k=495, n=495, Accuracy: 0.54 (+/- 0.03)\n",
      "k=496, n=496, Accuracy: 0.55 (+/- 0.03)\n",
      "k=497, n=497, Accuracy: 0.54 (+/- 0.03)\n",
      "k=498, n=498, Accuracy: 0.55 (+/- 0.02)\n",
      "k=499, n=499, Accuracy: 0.55 (+/- 0.03)\n",
      "k=500, n=500, Accuracy: 0.54 (+/- 0.03)\n",
      "k=501, n=501, Accuracy: 0.54 (+/- 0.02)\n",
      "k=502, n=502, Accuracy: 0.54 (+/- 0.03)\n",
      "k=503, n=503, Accuracy: 0.54 (+/- 0.02)\n",
      "k=504, n=504, Accuracy: 0.54 (+/- 0.03)\n",
      "k=505, n=505, Accuracy: 0.54 (+/- 0.03)\n",
      "k=506, n=506, Accuracy: 0.54 (+/- 0.03)\n",
      "k=507, n=507, Accuracy: 0.55 (+/- 0.02)\n",
      "k=508, n=508, Accuracy: 0.54 (+/- 0.03)\n",
      "k=509, n=509, Accuracy: 0.55 (+/- 0.03)\n",
      "k=510, n=510, Accuracy: 0.55 (+/- 0.03)\n",
      "k=511, n=511, Accuracy: 0.55 (+/- 0.03)\n",
      "k=512, n=512, Accuracy: 0.54 (+/- 0.03)\n",
      "k=513, n=513, Accuracy: 0.54 (+/- 0.03)\n",
      "k=514, n=514, Accuracy: 0.54 (+/- 0.02)\n",
      "k=515, n=515, Accuracy: 0.54 (+/- 0.03)\n",
      "k=516, n=516, Accuracy: 0.55 (+/- 0.03)\n",
      "k=517, n=517, Accuracy: 0.54 (+/- 0.02)\n",
      "k=518, n=518, Accuracy: 0.54 (+/- 0.02)\n",
      "k=519, n=519, Accuracy: 0.54 (+/- 0.03)\n",
      "k=520, n=520, Accuracy: 0.55 (+/- 0.02)\n",
      "k=521, n=521, Accuracy: 0.55 (+/- 0.02)\n",
      "k=522, n=522, Accuracy: 0.55 (+/- 0.01)\n",
      "k=523, n=523, Accuracy: 0.55 (+/- 0.02)\n",
      "k=524, n=524, Accuracy: 0.56 (+/- 0.03)\n",
      "k=525, n=525, Accuracy: 0.54 (+/- 0.03)\n",
      "k=526, n=526, Accuracy: 0.54 (+/- 0.03)\n",
      "k=527, n=527, Accuracy: 0.55 (+/- 0.03)\n",
      "k=528, n=528, Accuracy: 0.54 (+/- 0.03)\n",
      "k=529, n=529, Accuracy: 0.54 (+/- 0.03)\n",
      "k=530, n=530, Accuracy: 0.55 (+/- 0.03)\n",
      "k=531, n=531, Accuracy: 0.54 (+/- 0.03)\n",
      "k=532, n=532, Accuracy: 0.53 (+/- 0.03)\n",
      "k=533, n=533, Accuracy: 0.54 (+/- 0.02)\n",
      "k=534, n=534, Accuracy: 0.55 (+/- 0.03)\n",
      "k=535, n=535, Accuracy: 0.55 (+/- 0.03)\n",
      "k=536, n=536, Accuracy: 0.55 (+/- 0.02)\n",
      "k=537, n=537, Accuracy: 0.54 (+/- 0.02)\n",
      "k=538, n=538, Accuracy: 0.54 (+/- 0.03)\n",
      "k=539, n=539, Accuracy: 0.54 (+/- 0.02)\n",
      "k=540, n=540, Accuracy: 0.54 (+/- 0.02)\n",
      "k=541, n=541, Accuracy: 0.54 (+/- 0.02)\n",
      "k=542, n=542, Accuracy: 0.54 (+/- 0.03)\n",
      "k=543, n=543, Accuracy: 0.55 (+/- 0.03)\n",
      "k=544, n=544, Accuracy: 0.55 (+/- 0.03)\n",
      "k=545, n=545, Accuracy: 0.54 (+/- 0.03)\n",
      "k=546, n=546, Accuracy: 0.54 (+/- 0.02)\n",
      "k=547, n=547, Accuracy: 0.54 (+/- 0.03)\n",
      "k=548, n=548, Accuracy: 0.55 (+/- 0.03)\n",
      "k=549, n=549, Accuracy: 0.54 (+/- 0.03)\n",
      "k=550, n=550, Accuracy: 0.54 (+/- 0.02)\n",
      "k=551, n=551, Accuracy: 0.55 (+/- 0.03)\n",
      "k=552, n=552, Accuracy: 0.54 (+/- 0.03)\n",
      "k=553, n=553, Accuracy: 0.55 (+/- 0.03)\n",
      "k=554, n=554, Accuracy: 0.54 (+/- 0.03)\n",
      "k=555, n=555, Accuracy: 0.55 (+/- 0.02)\n",
      "k=556, n=556, Accuracy: 0.55 (+/- 0.02)\n",
      "k=557, n=557, Accuracy: 0.55 (+/- 0.02)\n",
      "k=558, n=558, Accuracy: 0.54 (+/- 0.03)\n",
      "k=559, n=559, Accuracy: 0.54 (+/- 0.03)\n",
      "k=560, n=560, Accuracy: 0.55 (+/- 0.02)\n",
      "k=561, n=561, Accuracy: 0.53 (+/- 0.03)\n",
      "k=562, n=562, Accuracy: 0.55 (+/- 0.03)\n",
      "k=563, n=563, Accuracy: 0.55 (+/- 0.02)\n",
      "k=564, n=564, Accuracy: 0.55 (+/- 0.02)\n",
      "k=565, n=565, Accuracy: 0.55 (+/- 0.03)\n",
      "k=566, n=566, Accuracy: 0.54 (+/- 0.02)\n",
      "k=567, n=567, Accuracy: 0.54 (+/- 0.03)\n",
      "k=568, n=568, Accuracy: 0.55 (+/- 0.02)\n",
      "k=569, n=569, Accuracy: 0.55 (+/- 0.02)\n",
      "k=570, n=570, Accuracy: 0.55 (+/- 0.03)\n",
      "k=571, n=571, Accuracy: 0.54 (+/- 0.03)\n",
      "k=572, n=572, Accuracy: 0.54 (+/- 0.03)\n",
      "k=573, n=573, Accuracy: 0.54 (+/- 0.02)\n",
      "k=574, n=574, Accuracy: 0.55 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "clf = RandomForestClassifier(random_state=0,class_weight={0: 1234/4774, 1: 1})\n",
    "\n",
    "# 計算每個特徵與目標變數的相關性\n",
    "F, p = f_regression(morgan_X_train, morgan_y_train)\n",
    "\n",
    "# 決定我們要試驗的特徵數量\n",
    "k_values = range(1, morgan_X_train.shape[1] + 1)\n",
    "\n",
    "best_score = 0\n",
    "best_k = 0\n",
    "best_X = None\n",
    "\n",
    "# 在迴圈開始前初始化兩個空列表\n",
    "n_list = []\n",
    "scores_list = []\n",
    "\n",
    "for k in k_values:\n",
    "    # 選擇 k 個最相關的特徵\n",
    "    selection = SelectKBest(f_regression, k=k)\n",
    "    select_X = selection.fit_transform(morgan_X_train, morgan_y_train)\n",
    "    \n",
    "    # 進行交叉驗證\n",
    "    scores = cross_val_score(clf, select_X, morgan_y_train, cv=5, scoring='f1')\n",
    "    mean_score = scores.mean()\n",
    "    \n",
    "    print(f\"k={k}, n={select_X.shape[1]}, Accuracy: {mean_score:.2f} (+/- {scores.std():.2f})\")\n",
    "\n",
    "    # 如果這個特徵數量的得分比我們之前找到的得分還高，則更新最佳特徵數量和數據集\n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_k = k\n",
    "        best_X = select_X\n",
    "\n",
    "    # 將每個特徵數量和對應的評分添加到列表中\n",
    "    n_list.append(select_X.shape[1])\n",
    "    scores_list.append(mean_score)\n",
    "\n",
    "print(f\"Best k={best_k}, Accuracy: {best_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688923e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed68ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90967eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best threshold={best_threshold}, Accuracy: {best_score:.2f}\")\n",
    "\n",
    "# 使用 matplotlib 繪製圖形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_list, scores_list, marker='o')\n",
    "plt.title('Score vs. Feature Importance Threshold')\n",
    "plt.xlabel('Feature Importance Threshold')\n",
    "plt.ylabel('Cross-Validated Score')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a0e562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = SelectKBest(score_func=mutual_info_classif, k= best_k)\n",
    "X_train_k = selection.fit_transform(morgan_X_train,morgan_y_train)\n",
    "X_test_k = selection.transform(morgan_X_test)\n",
    "print(np.array(X_train_k).shape,np.array(X_test_k).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2384bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "# PCA\n",
    "pca.fit(X_train_k)\n",
    "\n",
    "# 計算PCA各個主成份的方差解釋率\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# 累積方差解釋率\n",
    "cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# setting the threshold\n",
    "threshold = 0.95\n",
    "\n",
    "# 找到最小主成分数目\n",
    "n_components = np.where(cumulative_explained_variance_ratio >= threshold)[0][0] + 1\n",
    "\n",
    "print(f\"Number of components to preserve {threshold * 100}% variance: {n_components}\")\n",
    "\n",
    "pca_reduced = PCA(n_components=n_components)\n",
    "# 進行 PCA 轉換\n",
    "X_train = pca_reduced.fit_transform(X_train_k)\n",
    "# X_val1 = pca_reduced.transform(X_val)\n",
    "X_test = pca_reduced.transform(X_test_k)\n",
    "\n",
    "positive_indices = np.where(y_train == 1)[0]\n",
    "negative_indices = np.where(y_train == 0)[0]\n",
    "\n",
    "positive_samples = np.array(X_train)[positive_indices]\n",
    "negative_samples = np.array(X_train)[negative_indices]\n",
    "overlaps = []\n",
    "for i in range(positive_samples.shape[1]):  # for each feature\n",
    "    hist_pos, _ = np.histogram(positive_samples[:, i])\n",
    "    hist_neg, _ = np.histogram(negative_samples[:, i])\n",
    "    overlap = np.minimum(hist_pos, hist_neg).sum()  # calculate the overlap for this feature\n",
    "    overlaps.append(overlap)\n",
    "\n",
    "overlap_ratio = np.mean(overlaps) / positive_samples.shape[0]  # calculate the average overlap\n",
    "print(\"Average overlap ratio:\", overlap_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfd129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 初始化 k 折交叉驗證\n",
    "k = 5\n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "X = X_train_k\n",
    "y = morgan_y_train\n",
    "\n",
    "# 訓練和評估\n",
    "result_f1 = []\n",
    "result_mcc = []\n",
    "total_y_true = []\n",
    "total_y_pred = []\n",
    "\n",
    "f1_four = []\n",
    "mcc_four = []\n",
    "total_y_true_four = []\n",
    "total_y_pred_four = []\n",
    "for fold, (val_indices, test_indices) in enumerate(kfold.split(X, y)):\n",
    "    # 分割數據集\n",
    "    X_train, X_val = X[val_indices], X[test_indices]\n",
    "    y_train, y_val = y[val_indices], y[test_indices]\n",
    "    \n",
    "    # 將訓練集再分為訓練集和驗證集\n",
    "    #X_train1, X_val, y_train1, y_val = train_test_split(X_train1, y_train1, test_size=0.25, random_state=42, stratify=y_train1)\n",
    "    X_train = csr_matrix(X_train).toarray()\n",
    "\n",
    "\n",
    "    \n",
    "#     negative_indices = np.where(labels == 0)[0]\n",
    "#     negative_samples = np.array(np.array(feature))[negative_indices]\n",
    "#     negative_labels  = np.array(np.array(labels))[negative_indices]\n",
    "#     print(pd.Series(negative_labels).value_counts())\n",
    "\n",
    "\n",
    "    # 假设 X 是您的数据矩阵，形状为 (样本数, 特征数)，例如 (500, 2048)\n",
    "\n",
    "    # 初始化 PCA，所有主成分\n",
    "    pca = PCA()\n",
    "\n",
    "    # PCA\n",
    "    pca.fit(X_train)\n",
    "\n",
    "    # 計算PCA各個主成份的方差解釋率\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "    # 累積方差解釋率\n",
    "    cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "    # setting the threshold\n",
    "    threshold = 0.95\n",
    "\n",
    "    # 找到最小主成分数目\n",
    "    n_components = np.where(cumulative_explained_variance_ratio >= threshold)[0][0] + 1\n",
    "\n",
    "    print(f\"Number of components to preserve {threshold * 100}% variance: {n_components}\")\n",
    "    \n",
    "    pca_reduced = PCA(n_components=n_components)\n",
    "    # 進行 PCA 轉換\n",
    "    X_train = pca_reduced.fit_transform(X_train)\n",
    "    X_val = pca_reduced.transform(X_val)\n",
    "    X_test = pca_reduced.transform(morgan_X_test)\n",
    "#     X_four = pca_reduced.transform(np.array(feature))\n",
    "\n",
    "    # 轉換標籤為一維張量\n",
    "    y_train = y_train.flatten()\n",
    "    y_val1 = y_val.flatten()\n",
    "    y_test = morgan_y_train.flatten()\n",
    "#     y_four = labels.flatten()\n",
    "\n",
    "    # 創建數據加載器\n",
    "    train_data = torch.utils.data.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "    val_data = torch.utils.data.TensorDataset(torch.Tensor(X_val), torch.Tensor(y_val))\n",
    "    test_data = torch.utils.data.TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "#     four_data = torch.utils.data.TensorDataset(torch.Tensor(X_four), torch.Tensor(y_four))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=32, shuffle=True, drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size =32, shuffle=True, drop_last=True)\n",
    "#     four_loader = torch.utils.data.DataLoader(four_data, batch_size =32, shuffle=True, drop_last=True)\n",
    "\n",
    "    # 實例化模型、損失函數和優化器\n",
    "\n",
    "    input_size = 2048\n",
    "    embedding_dim = n_components\n",
    "    hidden_size = 128\n",
    "    output_size = 1 #二元分類問題\n",
    "    num_layers = 4\n",
    "    model = RNNModel(input_size,embedding_dim, hidden_size, output_size, num_layers).to(dtype=torch.float)\n",
    "\n",
    "    learning_rate = 0.0005\n",
    "\n",
    "    l2_lambda = 0.00005\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20, verbose=True)\n",
    "\n",
    "    patience = 30\n",
    "    counter = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "    n_epochs = 1000\n",
    "\n",
    "\n",
    "    # Initialize lists to store training and validation loss and F1 score\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_f1_scores = []\n",
    "    train_mcc_scores = []\n",
    "    val_f1_scores = []\n",
    "    val_mcc_scores = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss,t_f1,t_mcc,tcombined_score,train_mtr =  train(model, optimizer, train_loader)\n",
    "        val_loss, val_f1, val_mcc, combined_score,mtr = validate(model, val_loader, l2_lambda)\n",
    "        scheduler.step(val_loss)\n",
    "        # Append the results to the lists\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_f1_scores.append(t_f1)\n",
    "        train_mcc_scores.append(t_mcc)\n",
    "        val_f1_scores.append(val_f1)\n",
    "        val_mcc_scores.append(val_mcc)   \n",
    "        print('Epoch [{}/{}], Training Loss: {:.4f}, Training F1 score: {:.4f}, Training MCC score: {:.4f}, Training Combine score: {:.4f}'.format(epoch+1, n_epochs, train_loss,t_f1,t_mcc,tcombined_score))\n",
    "        print(train_mtr)\n",
    "        print('Epoch [{}/{}], Validation Loss: {:.4f}, Validation F1 score: {:.4f}, Validation MCC score: {:.4f}, Validation Combine score: {:.4f}'.format(epoch+1, n_epochs,val_loss, val_f1, val_mcc, combined_score))\n",
    "        print(mtr)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered at epoch {}.\".format(epoch+1))\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    test_loss, f1,mcc,mtr,y_true,y_pred = test(model, test_loader)\n",
    "    print('Test Loss: {:.4f}, F1 Score: {:.2f}, MCC: {:.2f}, Combine: {:.2f}'.format(test_loss, f1,mcc,(f1+mcc)/2))\n",
    "    print(mtr)\n",
    "    \n",
    "    total_y_pred = total_y_pred + y_pred\n",
    "    total_y_true = total_y_true + y_true\n",
    "    \n",
    "#     test_loss, f1,mcc,mtr,y_true,y_pred = test(model, four_loader)\n",
    "#     print('Test Loss: {:.4f}, Accuracy Score: {:.2f}'.format(test_loss,accuracy_score(y_true, y_pred)))\n",
    "#     print(mtr)\n",
    "\n",
    "#     total_y_pred_four = total_y_pred_four + y_pred\n",
    "#     total_y_true_four = total_y_true_four + y_true\n",
    "    \n",
    "    # Plotting the training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the training F1 score\n",
    "    plt.figure()\n",
    "    plt.plot(train_f1_scores, label='F1 Score', linestyle='-', marker='o')\n",
    "    plt.plot(train_mcc_scores, label='MCC Score', linestyle='--', marker='x')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Training F1 and MCC Scores')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the validation F1 score\n",
    "    plt.figure()\n",
    "    plt.plot(val_f1_scores, label='F1 Score', linestyle='-', marker='o')\n",
    "    plt.plot(val_mcc_scores, label='MCC Score', linestyle='--', marker='x')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Validation F1 and MCC Scores')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model_name = \"RNN\"\n",
    "dataset = \"MorganFingerPrint\"\n",
    "preprocessing = \"PCA\"\n",
    "per_index = performance(per_index,model_name,dataset,preprocessing,total_y_true,total_y_pred)\n",
    "# print(\"Testing for all negative : {}\".format(accuracy_score(total_y_true_four, total_y_pred_four)))\n",
    "per_index\n",
    "# model_name = \"RNN\"\n",
    "# dataset = \"MorganFingerPrint (ALL data)\"\n",
    "# preprocessing = \"PCA\"\n",
    "# per_index = performance(per_index,model_name,dataset,preprocessing,total_y_true_four,total_y_pred_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70704732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b57712f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8459b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 初始化 PCA，所有主成分\n",
    "pca = PCA()\n",
    "\n",
    "# PCA\n",
    "pca.fit(X_train_k)\n",
    "\n",
    "# 計算PCA各個主成份的方差解釋率\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# 累積方差解釋率\n",
    "cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# setting the threshold\n",
    "threshold = 0.95\n",
    "\n",
    "# 找到最小主成分数目\n",
    "n_components = np.where(cumulative_explained_variance_ratio >= threshold)[0][0] + 1\n",
    "\n",
    "print(f\"Number of components to preserve {threshold * 100}% variance: {n_components}\")\n",
    "\n",
    "pca_reduced = PCA(n_components=n_components)\n",
    "# 進行 PCA 轉換\n",
    "X_train = pca_reduced.fit_transform(X_train_k)\n",
    "# X_val1 = pca_reduced.transform(X_val)\n",
    "X_test = pca_reduced.transform(X_test_k)\n",
    "\n",
    "# 轉換標籤為一維張量\n",
    "y_train = morgan_y_train.ravel()\n",
    "y_test = np.array(morgan_y_test).ravel()\n",
    "# y_four = np.array(labels).ravel()\n",
    "print(\"Size of training set : {}   {}\".format(X_train.shape,y_train.shape))\n",
    "\n",
    "\n",
    "\n",
    "def mcc_score(y_true, y_pred):\n",
    "    return (matthews_corrcoef(y_true, y_pred)+f1_score(y_true, y_pred))/2\n",
    "\n",
    "# 将 MCC 转换为评估指标\n",
    "scoring = make_scorer(mcc_score)\n",
    "\n",
    "X_train_copy = X_train\n",
    "X_test_copy = X_test\n",
    "\n",
    "\n",
    "k_best_selector = SelectKBest(k=100)\n",
    "percentile_selector = SelectPercentile(percentile=100)\n",
    "pca_selector = PCA(n_components=100)\n",
    "selector = VarianceThreshold(threshold=0)\n",
    "svm = SVC(kernel='linear', C=1, random_state=42)\n",
    "selector = SelectFromModel(svm, threshold=0.3)\n",
    "rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold= \"mean\")\n",
    "n_iter_search = 20 \n",
    "\n",
    "\n",
    "\n",
    "nb_classifier = GaussianNB()\n",
    "params_nb = {'classifier__var_smoothing': np.logspace(0,-9, num=100)}\n",
    "pipeline_nb = Pipeline([\n",
    "    ('classifier', nb_classifier)\n",
    "])\n",
    "model_nb = RandomizedSearchCV(pipeline_nb, param_distributions=params_nb,\n",
    "                              n_iter=n_iter_search, verbose=1,\n",
    "                              scoring=scoring, cv=5)\n",
    "model_nb.fit(X_train_copy, y_train)\n",
    "nb = model_nb.best_estimator_\n",
    "matrix = metrics.confusion_matrix(y_test,nb.predict(X_test_copy))\n",
    "precision = precision_score(y_test,nb.predict(X_test_copy))\n",
    "recall = recall_score(y_test,nb.predict(X_test_copy))\n",
    "f1 = f1_score(y_test,nb.predict(X_test_copy))\n",
    "mcc = matthews_corrcoef(y_test,nb.predict(X_test_copy))\n",
    "\n",
    "model_name = \"GaussianNB\"\n",
    "dataset = \"MorganFingerPrint\"\n",
    "preprocessing = \"PCA\"\n",
    "per_index = performance(per_index,model_name,dataset,preprocessing,y_test,nb.predict(X_test_copy))\n",
    "\n",
    "print(\"########################NB########################\")\n",
    "print(\" Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "print(matrix)\n",
    "print(\"\\n\\n\")\n",
    "print(per_index)\n",
    "\n",
    "# matrix = metrics.confusion_matrix(y_four,nb.predict(X_four))\n",
    "# precision = precision_score(y_four,nb.predict(X_four))\n",
    "# recall = recall_score(y_four,nb.predict(X_four))\n",
    "# f1 = f1_score(y_four,nb.predict(X_four))\n",
    "# mcc = matthews_corrcoef(y_four,nb.predict(X_four))\n",
    "\n",
    "# model_name = \"GaussianNB\"\n",
    "# dataset = \"MorganFingerPrint (All data)\"\n",
    "# preprocessing = \"PCA\"\n",
    "# per_index = performance(per_index,model_name,dataset,preprocessing,y_four,nb.predict(X_four))\n",
    "\n",
    "# print(\"########################NB########################\")\n",
    "# print(\" Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "# print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "# print(matrix)\n",
    "# print(\"\\n\\n\")\n",
    "# print(per_index)\n",
    "\n",
    "\n",
    "X_train_copy = X_train\n",
    "X_test_copy = X_test\n",
    "\n",
    "params_log = {'classifier__penalty':['l2','l1'],\"classifier__C\" :[0.0001,0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    \"classifier__solver\":[\"liblinear\"],\"classifier__max_iter\":[50,100,200,500,700,1000],\n",
    "             \"classifier__class_weight\" : [{0: 1234/4774, 1: 1}]}\n",
    "log_classifier = LogisticRegression()\n",
    "pipeline_log = Pipeline([\n",
    "    ('classifier', log_classifier)\n",
    "])\n",
    "model_log = RandomizedSearchCV(pipeline_log,\n",
    "                               param_distributions=params_log,\n",
    "                               n_iter=n_iter_search, verbose=1,\n",
    "                               scoring=scoring, cv=5)\n",
    "model_log.fit(X_train_copy,y_train)\n",
    "logg = model_log.best_estimator_\n",
    "matrix = metrics.confusion_matrix(y_test,logg.predict(X_test_copy))\n",
    "precision = precision_score(y_test,logg.predict(X_test_copy))\n",
    "recall = recall_score(y_test,logg.predict(X_test_copy))\n",
    "f1 = f1_score(y_test,logg.predict(X_test_copy))\n",
    "mcc = matthews_corrcoef(y_test,logg.predict(X_test_copy))\n",
    "\n",
    "model_name = \"LogisticRegression\"\n",
    "dataset = \"MorganFingerPrint\"\n",
    "preprocessing = \"PCA\"\n",
    "per_index = performance(per_index,model_name,dataset,preprocessing,y_test,logg.predict(X_test_copy))\n",
    "\n",
    "print(\"########################LogisticRegression########################\")\n",
    "print(\"Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "print(matrix)\n",
    "print(\"\\n\\n\")\n",
    "print(per_index)\n",
    "\n",
    "# matrix = metrics.confusion_matrix(y_four,logg.predict(X_four))\n",
    "# precision = precision_score(y_four,logg.predict(X_four))\n",
    "# recall = recall_score(y_four,logg.predict(X_four))\n",
    "# f1 = f1_score(y_four,logg.predict(X_four))\n",
    "# mcc = matthews_corrcoef(y_four,logg.predict(X_four))\n",
    "\n",
    "# model_name = \"LogisticRegression\"\n",
    "# dataset = \"MorganFingerPrint (All data)\"\n",
    "# preprocessing = \"PCA\"\n",
    "# per_index = performance(per_index,model_name,dataset,preprocessing,y_four,logg.predict(X_four))\n",
    "\n",
    "# print(\"########################LogisticRegression########################\")\n",
    "# print(\"Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "# print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "# print(matrix)\n",
    "# print(\"\\n\\n\")\n",
    "# print(per_index)\n",
    "\n",
    "\n",
    "X_train_copy = X_train\n",
    "X_test_copy = X_test\n",
    "\n",
    "\n",
    "params_svm = {'classifier__kernel':['linear', 'poly', 'rbf','sigmoid'],\n",
    "              \"classifier__C\" :[0.0001,0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            \"classifier__gamma\" : ['scale','auto'],\n",
    "             \"classifier__class_weight\" : [{0: 1234/4774, 1: 1}]}\n",
    "svm_classifier = SVC()\n",
    "pipeline_svm = Pipeline([\n",
    "    ('classifier', svm_classifier)\n",
    "])\n",
    "model_svm = RandomizedSearchCV(pipeline_svm,\n",
    "                               param_distributions=params_svm,\n",
    "                               n_iter=n_iter_search, verbose=1,\n",
    "                               scoring=scoring, cv=5)\n",
    "model_svm.fit(X_train_copy,y_train)\n",
    "svm = model_svm.best_estimator_\n",
    "matrix = metrics.confusion_matrix(y_test,svm.predict(X_test_copy))\n",
    "precision = precision_score(y_test,svm.predict(X_test_copy))\n",
    "recall = recall_score(y_test,svm.predict(X_test_copy))\n",
    "f1 = f1_score(y_test,svm.predict(X_test_copy))\n",
    "mcc = matthews_corrcoef(y_test,svm.predict(X_test_copy))\n",
    "\n",
    "model_name = \"SVM\"\n",
    "dataset = \"MorganFingerPrint\"\n",
    "preprocessing = \"PCA\"\n",
    "per_index = performance(per_index,model_name,dataset,preprocessing,y_test,svm.predict(X_test_copy))\n",
    "\n",
    "print(\"########################SVM########################\")\n",
    "print(\"Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "print(matrix)\n",
    "print(\"\\n\\n\")\n",
    "print(per_index)\n",
    "# matrix = metrics.confusion_matrix(y_four,svm.predict(X_four))\n",
    "# precision = precision_score(y_four,svm.predict(X_four))\n",
    "# recall = recall_score(y_four,svm.predict(X_four))\n",
    "# f1 = f1_score(y_four,svm.predict(X_four))\n",
    "# mcc = matthews_corrcoef(y_four,svm.predict(X_four))\n",
    "\n",
    "# model_name = \"SVM\"\n",
    "# dataset = \"MorganFingerPrint (All data)\"\n",
    "# preprocessing = \"PCA\"\n",
    "# per_index = performance(per_index,model_name,dataset,preprocessing,y_four,svm.predict(X_four))\n",
    "\n",
    "# print(\"########################SVM########################\")\n",
    "# print(\"Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "# print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "# print(matrix)\n",
    "# print(\"\\n\\n\")\n",
    "# print(per_index)\n",
    "\n",
    "X_train_copy = X_train\n",
    "X_test_copy = X_test\n",
    "\n",
    "\n",
    "params_xgb = {'classifier__learning_rate':[0.01,0.001,0.0001],\n",
    "              'classifier__max_depth': range(10,100,20),\n",
    "              \"classifier__n_estimators\" :range(1,201,50),\n",
    "              \"classifier__gamma\" :range(1,10,3),\n",
    "              \"classifier__scale_pos_weight\":[(4774/1234)]\n",
    "             }\n",
    "xgb_classifier = XGBClassifier()\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('classifier', xgb_classifier)\n",
    "])\n",
    "model_xgb = RandomizedSearchCV(pipeline_xgb,\n",
    "                               param_distributions=params_xgb,\n",
    "                               n_iter=n_iter_search, verbose=1,\n",
    "                               scoring=scoring, cv=5)\n",
    "model_xgb.fit(X_train_copy,y_train)\n",
    "xgb = model_xgb.best_estimator_\n",
    "matrix = metrics.confusion_matrix(y_test,xgb.predict(X_test_copy))\n",
    "precision = precision_score(y_test,xgb.predict(X_test_copy),zero_division=0)\n",
    "recall = recall_score(y_test,xgb.predict(X_test_copy))\n",
    "f1 = f1_score(y_test,xgb.predict(X_test_copy))\n",
    "mcc = matthews_corrcoef(y_test,xgb.predict(X_test_copy))\n",
    "\n",
    "model_name = \"XGBClassifier\"\n",
    "dataset = \"MorganFingerPrint\"\n",
    "preprocessing = \"PCA\"\n",
    "per_index = performance(per_index,model_name,dataset,preprocessing,y_test,xgb.predict(X_test_copy))\n",
    "\n",
    "\n",
    "print(\"########################XGBClassifier########################\")\n",
    "print(\"Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "print(matrix)\n",
    "print(\"\\n\\n\")\n",
    "print(per_index)\n",
    "# matrix = metrics.confusion_matrix(y_four,xgb.predict(X_four))\n",
    "# precision = precision_score(y_four,xgb.predict(X_four),zero_division=0)\n",
    "# recall = recall_score(y_four,xgb.predict(X_four))\n",
    "# f1 = f1_score(y_four,xgb.predict(X_four))\n",
    "# mcc = matthews_corrcoef(y_four,xgb.predict(X_four))\n",
    "\n",
    "# model_name = \"XGBClassifier\"\n",
    "# dataset = \"MorganFingerPrint (All data)\"\n",
    "# preprocessing = \"PCA\"\n",
    "# per_index = performance(per_index,model_name,dataset,preprocessing,y_four,xgb.predict(X_four))\n",
    "\n",
    "\n",
    "# print(\"########################XGBClassifier########################\")\n",
    "# print(\"Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "# print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "# print(matrix)\n",
    "# print(\"\\n\\n\")\n",
    "# print(per_index)\n",
    "\n",
    "\n",
    "X_train_copy = X_train\n",
    "X_test_copy = X_test\n",
    "\n",
    "\n",
    "params_rf = {'classifier__max_features':['sqrt','log2'],\n",
    "              'classifier__max_depth': range(10,100,20),\n",
    "              \"classifier__n_estimators\" :range(100,1000,100),\n",
    "   \"classifier__criterion\" : ['gini','entropy'],\n",
    "             \"classifier__oob_score\" : [True],\n",
    "             \"classifier__class_weight\" : [{0: 1234/4774, 1: 1}]\n",
    "             \n",
    "             }\n",
    "rf_classifier = RandomForestClassifier()\n",
    "pipeline_rf = Pipeline([\n",
    "    ('classifier', rf_classifier)\n",
    "])\n",
    "model_rf = RandomizedSearchCV(pipeline_rf,\n",
    "                              param_distributions=params_rf,\n",
    "                              n_iter=n_iter_search, verbose=1,\n",
    "                              scoring=scoring, cv=5)\n",
    "model_rf.fit(X_train_copy,y_train)\n",
    "rf = model_rf.best_estimator_\n",
    "matrix = metrics.confusion_matrix(y_test,rf.predict(X_test_copy))\n",
    "precision = precision_score(y_test,rf.predict(X_test_copy),zero_division=1)\n",
    "recall = recall_score(y_test,rf.predict(X_test_copy))\n",
    "f1 = f1_score(y_test,rf.predict(X_test_copy))\n",
    "mcc = matthews_corrcoef(y_test,rf.predict(X_test_copy))\n",
    "\n",
    "model_name = \"RandomForestClassifier\"\n",
    "dataset = \"MorganFingerPrint\"\n",
    "preprocessing = \"PCA\"\n",
    "per_index = performance(per_index,model_name,dataset,preprocessing,y_test,rf.predict(X_test_copy))\n",
    "\n",
    "print(\"########################RandomForest########################\")\n",
    "print(\"Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "print(matrix)\n",
    "print(\"\\n\\n\")\n",
    "print(per_index)\n",
    "# matrix = metrics.confusion_matrix(y_four,rf.predict(X_four))\n",
    "# precision = precision_score(y_four,rf.predict(X_four),zero_division=1)\n",
    "# recall = recall_score(y_four,rf.predict(X_four))\n",
    "# f1 = f1_score(y_four,rf.predict(X_four))\n",
    "# mcc = matthews_corrcoef(y_four,rf.predict(X_four))\n",
    "\n",
    "# model_name = \"RandomForestClassifier\"\n",
    "# dataset = \"MorganFingerPrint (All data)\"\n",
    "# preprocessing = \"PCA\"\n",
    "# per_index = performance(per_index,model_name,dataset,preprocessing,y_four,rf.predict(X_four))\n",
    "\n",
    "# print(\"########################RandomForest########################\")\n",
    "# print(\"Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "# print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "# print(matrix)\n",
    "# print(\"\\n\\n\")\n",
    "# print(per_index)\n",
    "\n",
    "\n",
    "X_train_copy = X_train\n",
    "X_test_copy = X_test\n",
    "\n",
    "\n",
    "\n",
    "params_lgb = {'classifier__objective': [\"binary\"],\n",
    "              \"classifier__learning_rate\" :[0.01, 0.015, 0.025, 0.05, 0.1],\n",
    "   \"classifier__max_depth\" : [3, 5, 6, 7, 9, 12, 15, 17, 25],\n",
    "             \"classifier__subsample\" : [0.6, 0.7, 0.8, 0.9, 1],\n",
    "              \"classifier__class_weight\" : [{0: 1234/4774, 1: 1}]\n",
    "              \n",
    "             }\n",
    "lgb_classifier = lgb.LGBMClassifier()\n",
    "pipeline_lgb = Pipeline([\n",
    "    ('classifier', lgb_classifier)\n",
    "])\n",
    "model_lgb = RandomizedSearchCV(pipeline_lgb,\n",
    "                               param_distributions=params_lgb,\n",
    "                               n_iter=n_iter_search, verbose=1,\n",
    "                               scoring=scoring, cv=5)\n",
    "\n",
    "model_lgb.fit(X_train_copy,y_train)\n",
    "lgb = model_lgb.best_estimator_\n",
    "matrix = metrics.confusion_matrix(y_test,lgb.predict(X_test_copy))\n",
    "precision = precision_score(y_test,lgb.predict(X_test_copy))\n",
    "recall = recall_score(y_test,lgb.predict(X_test_copy))\n",
    "f1 = f1_score(y_test,lgb.predict(X_test_copy))\n",
    "mcc = matthews_corrcoef(y_test,lgb.predict(X_test_copy))\n",
    "\n",
    "model_name = \"LGBMClassifier\"\n",
    "dataset = \"MorganFingerPrint\"\n",
    "preprocessing = \"PCA\"\n",
    "per_index = performance(per_index,model_name,dataset,preprocessing,y_test,lgb.predict(X_test_copy))\n",
    "\n",
    "print(\"########################LGBMClassifier########################\")\n",
    "print(\"Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "print(matrix)\n",
    "print(\"\\n\\n\")\n",
    "print(per_index)\n",
    "# matrix = metrics.confusion_matrix(y_four,lgb.predict(X_four))\n",
    "# precision = precision_score(y_four,lgb.predict(X_four))\n",
    "# recall = recall_score(y_four,lgb.predict(X_four))\n",
    "# f1 = f1_score(y_four,lgb.predict(X_four))\n",
    "# mcc = matthews_corrcoef(y_four,lgb.predict(X_four))\n",
    "\n",
    "# model_name = \"LGBMClassifier\"\n",
    "# dataset = \"MorganFingerPrint (All data)\"\n",
    "# preprocessing = \"PCA\"\n",
    "# per_index = performance(per_index,model_name,dataset,preprocessing,y_four,lgb.predict(X_four))\n",
    "\n",
    "# print(\"########################LGBMClassifier########################\")\n",
    "# print(\"Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "# print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "# print(matrix)\n",
    "# print(\"\\n\\n\")\n",
    "# print(per_index)\n",
    "\n",
    "\n",
    "X_train_copy = X_train\n",
    "X_test_copy = X_test\n",
    "\n",
    "\n",
    "params_mlp = {'classifier__hidden_layer_sizes':[50,100,150,200],\n",
    "              'classifier__solver': [\"sgd\",\"adam\"],\n",
    "              \"classifier__learning_rate_init\" :[0.01, 0.015, 0.025, 0.05, 0.1],\n",
    "   \"classifier__early_stopping\" : [True],\n",
    "             \"classifier__shuffle\" : [True],\n",
    "              \"classifier__class_weight\" : [{0: 1234/4774, 1: 1}]\n",
    "             }\n",
    "\n",
    "mlp_classifier= MLPClassifier()\n",
    "pipeline_mlp = Pipeline([\n",
    "    ('classifier', mlp_classifier)\n",
    "])\n",
    "model_mlp = RandomizedSearchCV(pipeline_mlp,\n",
    "                               param_distributions=params_mlp,\n",
    "                               n_iter=n_iter_search, verbose=1,\n",
    "                               scoring=scoring, cv=5)\n",
    "model_mlp.fit(X_train_copy,y_train)\n",
    "mlp = model_mlp.best_estimator_\n",
    "matrix = metrics.confusion_matrix(y_test,mlp.predict(X_test_copy))\n",
    "precision = precision_score(y_test,mlp.predict(X_test_copy),zero_division = 1)\n",
    "recall = recall_score(y_test,mlp.predict(X_test_copy))\n",
    "f1 = f1_score(y_test,mlp.predict(X_test_copy))\n",
    "mcc = matthews_corrcoef(y_test,mlp.predict(X_test_copy))\n",
    "\n",
    "model_name = \"MLPClassifier\"\n",
    "dataset = \"MorganFingerPrint\"\n",
    "preprocessing = \"PCA\"\n",
    "per_index = performance(per_index,model_name,dataset,preprocessing,y_test,mlp.predict(X_test_copy))\n",
    "\n",
    "print(\"########################MLPClassifier########################\")\n",
    "print(\"Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "print(matrix)\n",
    "print(\"\\n\\n\")\n",
    "print(per_index)\n",
    "# matrix = metrics.confusion_matrix(y_four,mlp.predict(X_four))\n",
    "# precision = precision_score(y_four,mlp.predict(X_four),zero_division = 1)\n",
    "# recall = recall_score(y_four,mlp.predict(X_four))\n",
    "# f1 = f1_score(y_four,mlp.predict(X_four))\n",
    "# mcc = matthews_corrcoef(y_four,mlp.predict(X_four))\n",
    "\n",
    "# model_name = \"MLPClassifier\"\n",
    "# dataset = \"MorganFingerPrint (All data)\"\n",
    "# preprocessing = \"PCA\"\n",
    "# per_index = performance(per_index,model_name,dataset,preprocessing,y_four,mlp.predict(X_four))\n",
    "\n",
    "# print(\"########################MLPClassifier########################\")\n",
    "# print(\"Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "# print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "# print(matrix)\n",
    "# print(\"\\n\\n\")\n",
    "# print(per_index)\n",
    "\n",
    "\n",
    "X_train_copy = X_train\n",
    "X_test_copy = X_test\n",
    "\n",
    "\n",
    "params_cat = {'classifier__loss_function':[\"RMSE\",\"Logloss\",\"CrossEntropy\",\"MAE\"],\n",
    "              'classifier__iterations': [1000],\n",
    "              \"classifier__learning_rate\" :[0.01, 0.015, 0.025, 0.05, 0.1],\n",
    "             \"classifier__sampling_frequency\" : [\"PerTree\",\"PerTreeLevel\"]\n",
    "             }\n",
    "cat_classifier = CatBoostClassifier()\n",
    "pipeline_cat = Pipeline([\n",
    "    ('classifier', cat_classifier)\n",
    "])\n",
    "model_cat = RandomizedSearchCV(pipeline_cat,\n",
    "                               param_distributions=params_cat,\n",
    "                               n_iter=n_iter_search, verbose=0,\n",
    "                               scoring=scoring, cv=5)\n",
    "model_cat.fit(X_train_copy,y_train)\n",
    "cat = model_cat.best_estimator_\n",
    "matrix = metrics.confusion_matrix(y_test,cat.predict(X_test_copy))\n",
    "precision = precision_score(y_test,cat.predict(X_test_copy))\n",
    "recall = recall_score(y_test,cat.predict(X_test_copy))\n",
    "f1 = f1_score(y_test,cat.predict(X_test_copy))\n",
    "mcc = matthews_corrcoef(y_test,cat.predict(X_test_copy))\n",
    "\n",
    "model_name = \"CatBoostClassifier\"\n",
    "dataset = \"MorganFingerPrint\"\n",
    "preprocessing = \"PCA\"\n",
    "per_index = performance(per_index,model_name,dataset,preprocessing,y_test,cat.predict(X_test_copy))\n",
    "\n",
    "print(\"########################CatBoostClassifier########################\")\n",
    "print(\"Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "print(matrix)\n",
    "print(\"\\n\\n\")\n",
    "print(per_index)\n",
    "# matrix = metrics.confusion_matrix(y_four,cat.predict(X_four))\n",
    "# precision = precision_score(y_four,cat.predict(X_four))\n",
    "# recall = recall_score(y_four,cat.predict(X_four))\n",
    "# f1 = f1_score(y_four,cat.predict(X_four))\n",
    "# mcc = matthews_corrcoef(y_four,cat.predict(X_four))\n",
    "\n",
    "# model_name = \"CatBoostClassifier\"\n",
    "# dataset = \"MorganFingerPrint (All data)\"\n",
    "# preprocessing = \"PCA\"\n",
    "# per_index = performance(per_index,model_name,dataset,preprocessing,y_four,cat.predict(X_four))\n",
    "\n",
    "# print(\"########################CatBoostClassifier########################\")\n",
    "# print(\"Best F1 score {} , MCC : {}\".format(f1,mcc))\n",
    "# print(\"Precision : {},  Recall : {}\".format(precision,recall))\n",
    "# print(matrix)\n",
    "# print(\"\\n\\n\")\n",
    "# print(per_index)\n",
    "per_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b9ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = per_index\n",
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad428b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d03ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6db6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d70e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
